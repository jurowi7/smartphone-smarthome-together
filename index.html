<!DOCTYPE html>
<html>
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <link href='https://fonts.googleapis.com/css?family=Architects+Daughter' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/print.css" media="print">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <title>Smartphone-smarthome-together by jurowi7</title>
  </head>

  <body>
    <header>
      <div class="inner">
        <h1>Smartphone and Smarthome Data Working Together</h1>
        <h2>Using smartphone and smarthome data to predict someone&#39;s actions</h2>
        <a href="https://github.com/jurowi7/smartphone-smarthome-together" class="button"><small>View project on</small> GitHub</a>
      </div>
    </header>

    <div id="content-wrapper">
      <div class="inner clearfix">
        <section id="main-content">

        <h3>Combining Data</h3>
        <p>In smartphones the accelerometer and gyroscope allow you to capture the acceleration relative to the x-axis, y-axis, and z-axis as well as the angle of rotation around each axis. You can create training sets where you train the phone to identify motions such as sitting, walking, or laying down while the user has their smartphone in their pocket and record the movement data for comparison later.</p>
        <p>In a smarthome environment you can collect data from motion sensors in each room to identify which room is being occupied. Energy use data from appliances can identify what the user is interacting with such as a microwave, washer, or TV. 
        <p>By combining the data from these two you can potentially identify a user's actions and location within a smarthome environment. For example you could predict something such as a user standing in the kitchen using the microwave.</p>

        <h3>Predicting Action</h3>
        <p>Currently the program is set up to predict motion, the phone does not actively gather data from smarthome devices. When in the predict action mode, every movement has its data compared against the training sets mentioned earlier using the k-nearest neighbor algorithm. In this case I have it finding the 3 closest matches then choosing a predicted action by looking at which action is found the most in those matches.</p>
        <p>You can adjust the number of k-nearest neighbor matches to search as well as the frequency of motion data being saved to get more accurate results (though potentially slower response because of increased data and processing).</p>

        <h3>Potential Uses</h3>
        <p>This can be useful to monitor those that may need assistance. A caregiver could be given live data to monitor activities which in turn could help chart progress of patients, know their locations, and aid in preventing or identifying potential problems.</p>

        <h3>Video Walkthrough</h3>
        <p><video width="800" height="450" controls><source src="videos/Predicting Actions.mp4" type="video/mp4"></video></p>

        <h3>App Screens</h3>
        <br><br>

        <table>
          <tr>
            <td> 
              <p><img src="images/screen1.jpg"></p>
            </td>

            <td>
              <h3>Main Menu</h3>
              <p>There are two core parts to this app: adding movement data and predicting a movement. I've also included an option to view the movement action data stored on the phone.</p>
            </td>
          </tr>

          <tr>
            <td>
              <p><img src="images/screen2.jpg"></p>
            </td>

            <td>
              <h3>Add Action Data</h3>
              <p>Create a name for the action being recorded. You can then choose how long to record the action and the data retrieval rate.</p> 
              <p>When storing the data directly to the iPhone it may not work as expected if you choose to have a data retrieval rate more than 10 times per second since it may not be able to keep up the process of storing the data that frequently.</p> 
              <p>A one second start delay was added so that you have some time to get in position after pressing the start button.</p>
            </td>
          </tr>

          <tr>
            <td>
              <p><img src="images/screen3.jpg"></p>
            </td>

            <td>
              <h3>Recording Data</h3>
              <p>Displays the action name you entered as well as the accelration and rotation of the x, y, and z axes.</p> 
              <p>The accelerometer and gyroscope are both set to update using the interval listed on the previous screen (10 times per second by default).</p>
              <p>The app will save the data every time those values are updated and it will stop recording after the duration specified (2 seconds by default).</p>
            </td>
          </tr>

          <tr>
            <td>
              <p><img src="images/screen4.jpg"></p><br>
            </td>

            <td>
              <h3>Viewing Data</h3>
              <p>After action data has been stored you can choose to view that data.</p> 
              <p>The data stored includes a timestamp to the nearest hundredth of a second, accleration of the x, y, and z axes, and the rotational data (yaw, pitch, and roll) around the x, y, and z axes respectively.</p>
            </td>
          </tr>

          <tr>
            <td>
              <p><img src="images/screen5.jpg"></p>
            </td>

            <td>
              <h3>Predicting Action</h3>
              <p>Displays the number ot training sets (rows of action data) that have been saved. This is the data compared against when predicting a user's action.</p> 
              <p>The predicting movement section is updated 10 times per second using the current accelerometer and gyroscope data compared to the data stored in the training sets.</p>
              <p>The last two fields, predicted location and predicted action, are hypothetical since they would be populated from data gathered in a smart home environment.</p>
            </td>
          </tr>
        </table>

        <h3>Future Work</h3>

        <p>By adding another smart wearable device such as a smartwatch you could gather even more data relating to someones health or actions. You would have data from arm motions as well as their heart rate. There's also a potential for using this to track children and their activities, though for practical use a different design of wearable technology may be needed since it is uncommon for infants and young children to have smartphones or smartwatches.</p>

        </section>

        <aside id="sidebar">
          <a href="https://github.com/jurowi7/smartphone-smarthome-together/zipball/master" class="button">
            <small>Download</small>
            .zip file
          </a>
          <a href="https://github.com/jurowi7/smartphone-smarthome-together/tarball/master" class="button">
            <small>Download</small>
            .tar.gz file
          </a>

          <p class="repo-owner"><a href="https://github.com/jurowi7/smartphone-smarthome-together"></a> Maintained by <a href="https://github.com/jurowi7">Justin Wilkinson</a>.</p>

          <p>This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the Architect theme by <a href="https://twitter.com/jasonlong">Jason Long</a>.</p>
        </aside>
      </div>
    </div>
  </body>
</html>
